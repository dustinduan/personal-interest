{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络爬取之前的贮备工作\n",
    "    检查robet.txt文件\n",
    "    检查sitemap文件内容\n",
    "    估算网站大小（渐变方法是通过google搜索site：+网络地址的方法来粗略确认网站的大小）\n",
    "    识别网站使用的技术（builtwith.parse(网络主机地址)）可以显示网站的信息,见下面的例子\n",
    "    寻找网站所有者（通过whois模块）\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nginx']\n",
      "['Web2py', 'Twitter Bootstrap']\n",
      "['Python']\n",
      "['jQuery', 'Modernizr', 'jQuery UI']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WEBSCRAPING.COM\n",
      "GoDaddy.com, LLC\n",
      "whois.godaddy.com\n",
      "None\n",
      "[datetime.datetime(2013, 8, 20, 8, 8, 30), datetime.datetime(2013, 8, 20, 8, 8, 29)]\n",
      "2004-06-26 18:01:19\n",
      "2020-06-26 18:01:19\n",
      "['NS1.WEBFACTION.COM', 'NS2.WEBFACTION.COM', 'NS3.WEBFACTION.COM', 'NS4.WEBFACTION.COM']\n",
      "['clientDeleteProhibited https://icann.org/epp#clientDeleteProhibited', 'clientRenewProhibited https://icann.org/epp#clientRenewProhibited', 'clientTransferProhibited https://icann.org/epp#clientTransferProhibited', 'clientUpdateProhibited https://icann.org/epp#clientUpdateProhibited', 'clientTransferProhibited http://www.icann.org/epp#clientTransferProhibited', 'clientUpdateProhibited http://www.icann.org/epp#clientUpdateProhibited', 'clientRenewProhibited http://www.icann.org/epp#clientRenewProhibited', 'clientDeleteProhibited http://www.icann.org/epp#clientDeleteProhibited']\n",
      "abuse@godaddy.com\n",
      "unsigned\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Victoria\n",
      "None\n",
      "AU\n"
     ]
    }
   ],
   "source": [
    "import builtwith\n",
    "import whois\n",
    "website='http://example.webscraping.com'\n",
    "data=builtwith.parse(website)\n",
    "for i in data:\n",
    "    print(data[i])\n",
    "print('\\n\\n\\n')\n",
    "data=whois.whois(website)\n",
    "for i in data:\n",
    "    print(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写第一个网络爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习设置网络代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://www.xbiquge.la/10/10489/9683462.html\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as ul\n",
    "from bs4 import BeautifulSoup as bl\n",
    "import json\n",
    "def download(website,user_agent='wswp',num_retries=3):#增加自动重新下载的参数num_retries，默认数量为3,user_agent为设置代理\n",
    "    print(\"Downloading:\",website)\n",
    "    headers={'User-agent':user_agent}\n",
    "    request=ul.Request(website,headers=headers)\n",
    "    try:\n",
    "        html=ul.urlopen(website).read()\n",
    "    except ul.URLError as e:\n",
    "        print(\"download Error:\",e.reason)\n",
    "        html=None\n",
    "        if hasattr(e,'code') and 500<=e.code<=600:\n",
    "            return(download(website,num_retries-1))\n",
    "    return (html)\n",
    "website='http://www.xbiquge.la/10/10489/9683462.html'\n",
    "html=bl(download(website),'lxml')\n",
    "infors=html.find_all('div',id_='content')\n",
    "print(infors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习使用遍历爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com/view/-1\n",
      "http://example.webscraping.com/view/-1\n",
      "\n",
      "\n",
      "Downloading: http://example.webscraping.com/view/-2\n",
      "http://example.webscraping.com/view/-2\n",
      "\n",
      "\n",
      "Downloading: http://example.webscraping.com/view/-3\n",
      "http://example.webscraping.com/view/-3\n",
      "\n",
      "\n",
      "Downloading: http://example.webscraping.com/view/-4\n",
      "http://example.webscraping.com/view/-4\n",
      "\n",
      "\n",
      "Downloading: http://example.webscraping.com/view/-5\n",
      "http://example.webscraping.com/view/-5\n",
      "\n",
      "\n",
      "Downloading: http://example.webscraping.com/view/-6\n"
     ]
    }
   ],
   "source": [
    "import itertools as it\n",
    "for page in it.count(1):\n",
    "    url='http://example.webscraping.com/view/-%d'% page\n",
    "    html=download(url)\n",
    "    if html is None:\n",
    "        break\n",
    "    else:\n",
    "        print(url)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "用来爬取链接的简单爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://example.webscraping.com\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4b8e1734a0e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mlink_crawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"http://example.webscraping.com\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/(index/view)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-4b8e1734a0e0>\u001b[0m in \u001b[0;36mlink_crawler\u001b[1;34m(seed_url, link_regex)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcrawl_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mhtml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink_regex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mcrawl_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-4b8e1734a0e0>\u001b[0m in \u001b[0;36mget_links\u001b[1;34m(html)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mwebpage_regex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<a[^>]+href=[\"\\'](.*?)[\"\\']'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwebpage_regex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlink_crawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlink_regex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def get_links(html):\n",
    "    webpage_regex=re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']',re.IGNORECASE)\n",
    "    return(webpage_regex.findall(html))\n",
    "\n",
    "def link_crawler(seed_url,link_regex):\n",
    "    crawl_queue=[seed_url]\n",
    "    while crawl_queue:\n",
    "        url=crawl_queue.pop()\n",
    "        html=download(url)\n",
    "        for link in get_links(html):\n",
    "            if re.match(link_regex,link):\n",
    "                crawl_queue.append(link)\n",
    "    list=set(crawl_queue)\n",
    "    print(list)\n",
    "\n",
    "link_crawler(\"http://example.webscraping.com\",'/(index/view)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入需要查询的IP地址:223.5.5.5\n",
      "IP地址对应的信息如下:\n",
      "\n",
      "ret_code 0\n",
      "continents 亚洲\n",
      "country 中国\n",
      "region 浙江\n",
      "city 杭州\n",
      "county \n",
      "isp AliDNS\n",
      "city_code 330100\n",
      "en_name China\n",
      "en_name_short CN\n",
      "lnt 120.153576\n",
      "lat 30.287459\n",
      "ip 223.5.5.5\n",
      "请按任意键结束查询。。。。。。\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import urllib.request as ul\n",
    "\n",
    "\n",
    "def ip_search(ip):\n",
    "    host = 'http://saip.market.alicloudapi.com'\n",
    "    path = '/ip'\n",
    "    method = 'GET'\n",
    "    appcode = '91fa754784494761b450672aba2d3efb'\n",
    "    querys = 'ip='+ip\n",
    "    bodys = {}\n",
    "    url = host + path + '?' + querys\n",
    "    head={'Authorization':'APPCODE '+appcode}\n",
    "    request = ul.Request(url,headers=head)\n",
    "    response = ul.urlopen(request)\n",
    "    content = response.read()\n",
    "    info=json.loads(content)\n",
    "    return(info)\n",
    "ip=input('请输入需要查询的IP地址:')\n",
    "info=ip_search(ip)\n",
    "k=info['showapi_res_body']\n",
    "print('IP地址对应的信息如下:'+'\\n')\n",
    "for i in k:\n",
    "    print(i,k[i])\n",
    "c=input('请按任意键结束查询。。。。。。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
